{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=0gZ-l0npPIca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n",
    "# pip install torch-geometric\n",
    "\n",
    "# kobert (https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf)\n",
    "# pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/anaconda3/envs/jy_gpu_py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For GNN ================================================================================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "#import scipy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from typing import Optional, Tuple\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "# ========================================================================================\n",
    "\n",
    "# For Description Encoding ===============================================================\n",
    "import re\n",
    "\n",
    "from transformers import BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "bert_encoder = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "# ========================================================================================\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "random.seed(0)\n",
    "# with open(\"file_info.json\", 'r') as f :\n",
    "#     file_info = json.load(f)\n",
    "    \n",
    "names= ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "        'hair drier', 'toothbrush']\n",
    "\n",
    "\n",
    "with open(\"visual_features_daily_all.pickle\", 'rb') as f :\n",
    "    visual_features_daily = pickle.load(f)\n",
    "    \n",
    "with open(\"visual_features_diagnosis_all.pickle\", 'rb') as f :\n",
    "    visual_features_diagnosis = pickle.load(f)\n",
    "    \n",
    "with open(\"metadata_features_daily_all.pickle\", 'rb') as f :\n",
    "    meta_daily_features = pickle.load(f)\n",
    "with open(\"metadata_features_diagnosis_all.pickle\", 'rb') as f :\n",
    "    meat_diagnosis_features = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = meta_daily_features.copy()\n",
    "meta_features.update(meat_diagnosis_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4107, 1877, 2230)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta_features), len(meta_daily_features), len(meat_diagnosis_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/media/dxlab/storage/junyeop/'\n",
    "#len(os.listdir(PATH + \"image_dep\")), len(os.listdir(PATH + \"image_daily\"))\n",
    "\n",
    "adj_daily = os.listdir(PATH + \"adj_mat/daily/\")\n",
    "adj_depression = os.listdir(PATH + \"adj_mat/depression/\")\n",
    "adj_diagnosis = os.listdir(PATH + \"adj_mat/diagnosis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4107, 1877, 2230)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PATH + 'vlog_metadata_diagnosis.pickle', 'rb') as f :\n",
    "    meta_diagnosis = pickle.load(f)\n",
    "\n",
    "with open(PATH + 'vlog_metadata_daily.pickle', 'rb') as f :\n",
    "    meta_daily = pickle.load(f)\n",
    "with open(PATH + 'vlog_metadata_daily2.pickle', 'rb') as f :\n",
    "    meta_daily2= pickle.load(f)    \n",
    "meta_daily.update(meta_daily2)\n",
    "\n",
    "file_info = dict()\n",
    "\n",
    "have_meta_daily = dict()\n",
    "for i in adj_daily :\n",
    "    if os.path.splitext(i)[0] in list(meta_daily.keys()) :\n",
    "        if type(meta_daily[os.path.splitext(i)[0]]) == str :\n",
    "            continue\n",
    "        have_meta_daily[os.path.splitext(i)[0]] = meta_daily[os.path.splitext(i)[0]]\n",
    "        have_meta_daily[os.path.splitext(i)[0]]['label'] = 0\n",
    "        try :\n",
    "            have_meta_daily[os.path.splitext(i)[0]]['visual_feature'] = visual_features_daily[os.path.splitext(i)[0]]\n",
    "        except :\n",
    "            del have_meta_daily[os.path.splitext(i)[0]]                  \n",
    "        \n",
    "have_meta_diagnosis = dict()\n",
    "for i in adj_diagnosis :\n",
    "    if os.path.splitext(i)[0] in list(meta_diagnosis.keys()) :\n",
    "        if type(meta_diagnosis[os.path.splitext(i)[0]]) == str :\n",
    "            continue        \n",
    "        have_meta_diagnosis[os.path.splitext(i)[0]] = meta_diagnosis[os.path.splitext(i)[0]]\n",
    "        have_meta_diagnosis[os.path.splitext(i)[0]]['label'] = 1\n",
    "        try :\n",
    "            have_meta_diagnosis[os.path.splitext(i)[0]]['visual_feature'] = visual_features_diagnosis[os.path.splitext(i)[0]]\n",
    "        except :\n",
    "            del have_meta_diagnosis[os.path.splitext(i)[0]]\n",
    "            \n",
    "file_info = dict()\n",
    "file_info.update(have_meta_daily)\n",
    "file_info.update(have_meta_diagnosis)\n",
    "\n",
    "len(file_info), len(have_meta_daily), len(have_meta_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_ATTR = 80\n",
    "HIDDEN_DIM =  1024\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "MAX_LENGTH_DESCRIPTION = 100#max([len(file_info[x]['description']) for x in file_info])\n",
    "MAX_LENGTH_TITLE = max([len(x) for x in file_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = torch.eye(len(names), dtype = torch.float32) # node feature matrix all one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily vlog : 1877\n",
      "dep vlog : 2230\n"
     ]
    }
   ],
   "source": [
    "labels = list()\n",
    "c0, c1, c2 = 0, 0, 0\n",
    "\n",
    "for x in file_info :\n",
    "    if file_info[x]['label'] == 0:\n",
    "        labels.append(0)\n",
    "        c0 += 1\n",
    "    elif file_info[x]['label'] == 1 :\n",
    "        labels.append(1)\n",
    "        c1 += 1\n",
    "    else :\n",
    "        labels.append(2)\n",
    "        c2 += 1 \n",
    "labels = torch.LongTensor(labels).unsqueeze(dim = 1)\n",
    "print(\"daily vlog : {}\\ndep vlog : {}\".format(c0, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4107it [01:16, 53.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = list()\n",
    "\n",
    "\n",
    "for idx, vlog in tqdm(enumerate(file_info)) :\n",
    "\n",
    "    if labels[idx] == 0 :\n",
    "        with open(PATH + \"adj_mat/daily/\" + vlog + '.npy', 'rb') as f:\n",
    "            m = np.load(f)\n",
    "    #elif labels[idx] == 1:\n",
    "    #    with open(PATH + \"adj_mat/depression/\" + vlog + '.npy', 'rb') as f:\n",
    "    #        m = np.load(f)\n",
    "    else :\n",
    "        with open(PATH + \"adj_mat/diagnosis/\" + vlog + '.npy', 'rb') as f:\n",
    "            m = np.load(f)        \n",
    "    m = nx.adjacency_matrix(nx.from_numpy_array(m))\n",
    "    edge_index, edge_attr = torch_geometric.utils.from_scipy_sparse_matrix(m)\n",
    "    data = Data(edge_index=edge_index, x=feature_matrix, edge_attr = edge_attr, y = labels[idx])\n",
    "    \n",
    "    if 'description' in file_info[vlog] :\n",
    "        dataset.append([data, \n",
    "                        file_info[vlog]['visual_feature'], \n",
    "                        meta_features[vlog]['title'], \n",
    "                        meta_features[vlog]['description'], \n",
    "                        meta_features[vlog]['duration']])\n",
    "random.seed(0)    \n",
    "random.shuffle(dataset)\n",
    "\n",
    "\n",
    "# dataset format\n",
    "# dataset = [graph input, title token, description token, duration]\n",
    "\n",
    "# with open(\"full_dataset_with_graph_meta.pickle\", 'wb') as f:\n",
    "#     pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily 평균 길이 : 903.3958444326053\n",
      " depression 평균 길이 : 416.0340807174888\n"
     ]
    }
   ],
   "source": [
    "# check duration average\n",
    "daily_duration, daily_count = 0, 0\n",
    "depression_duration, depression_count = 0, 0\n",
    "\n",
    "\n",
    "for x in dataset :\n",
    "    if x[0].y.item() == 0:\n",
    "        daily_count += 1\n",
    "        daily_duration += x[-1].item()\n",
    "    else :\n",
    "        depression_count += 1\n",
    "        depression_duration += x[-1].item()\n",
    "print(\"daily 평균 길이 : {}\\n depression 평균 길이 : {}\".format(daily_duration/daily_count, depression_duration/depression_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
    "valid_dataset = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
    "test_dataset = dataset[int(len(dataset)*0.9):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "y_train = [x[0].y for x in train_dataset]\n",
    "y_valid = [x[0].y for x in valid_dataset]\n",
    "y_test = [x[0].y for x in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "y_test = [x[0].y for x in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1519, tensor([1766]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train) - sum(y_train).item(), sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3285, 411, 411)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily vlog : 1519\n",
      "dep vlog : 1766\n"
     ]
    }
   ],
   "source": [
    "c0, c1, c2 = 0, 0, 0\n",
    "\n",
    "for x in y_train :\n",
    "    if x == 0:\n",
    "        \n",
    "        c0 += 1\n",
    "    else :\n",
    "        c1 += 1\n",
    "\n",
    "print(\"daily vlog : {}\\ndep vlog : {}\".format(c0, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = HIDDEN_DIM, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
    "\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
    "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \n",
    "        batch_size = value.size(0)\n",
    "\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n",
    "\n",
    "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
    "\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n",
    "\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(NODE_ATTR, HIDDEN_DIM)\n",
    "        self.conv2 = GATConv(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.conv3 = GATConv(HIDDEN_DIM, HIDDEN_DIM)        \n",
    "        \n",
    "        self.self_attn = MultiHeadAttention()\n",
    "        self.cross_attn = MultiHeadAttention()\n",
    "    \n",
    "        self.visual_linear = nn.Linear(1000, HIDDEN_DIM)   \n",
    "    \n",
    "        # reduce dim\n",
    "        self.title_linear = nn.Linear(768, HIDDEN_DIM//2)\n",
    "        self.description_linear = nn.Linear(768, HIDDEN_DIM//2)\n",
    "        \n",
    "        self.dim_reduce = nn.Linear(2 * HIDDEN_DIM + 1, HIDDEN_DIM)\n",
    "        \n",
    "        self.lin1 = Linear(HIDDEN_DIM, HIDDEN_DIM//2)\n",
    "        self.lin2 = Linear(HIDDEN_DIM//2, HIDDEN_DIM//4)\n",
    "        \n",
    "        self.classifier = Linear(HIDDEN_DIM//4, len(labels.unique()))\n",
    "            \n",
    "    def forward(self, x, edge_index, batch, visual_feature, title, description, duration):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = self.self_attn(x, x, x)[0]     \n",
    "        \n",
    "        x_visual = self.visual_linear(visual_feature).squeeze(dim = 1)\n",
    "        \n",
    "        description = self.description_linear(description).squeeze(dim = 1)\n",
    "        title = self.title_linear(title).squeeze(dim = 1)\n",
    "        duration = duration\n",
    "        #print(x_visual.shape, description.shape, title.shape, duration.shape)\n",
    "        x_all = torch.cat([x_visual, description, title, duration], dim = 1)\n",
    "        x_all = self.dim_reduce(x_all)\n",
    "        \n",
    "        x = self.cross_attn(x, x_all, x_all)[0]\n",
    "        \n",
    "        x = F.dropout(self.lin1(x), p=0.5, training=self.training)\n",
    "        x = F.dropout(self.lin2(x), p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#for p in bert_encoder.parameters() :\n",
    "#    p.requires_grad = False\n",
    "\n",
    "#bert_encoder = bert_encoder.to(device)\n",
    "    \n",
    "model= Model()\n",
    "MODEL_PATH = './checkpoint/grid_search/0_binary_depression_multimodal/GAT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(opt):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    for data, visual_feature, title, description, duration in train_loader: # Iterate in batches over the training dataset.\n",
    "        \n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "\n",
    "        visual_feature = visual_feature.to(device)\n",
    "        title = title.to(device)\n",
    "        description = description.to(device)\n",
    "        duration = duration.to(device)\n",
    "\n",
    "#        description = bert_encoder(description).pooler_output\n",
    "#        title = bert_encoder(title).pooler_output   \n",
    "\n",
    "        out = model(x = data.x, \n",
    "                    edge_index = data.edge_index, \n",
    "                    batch = data.batch, \n",
    "                    visual_feature = visual_feature,\n",
    "                    title = title, \n",
    "                    description = description, \n",
    "                    duration = duration)  # Perform a single forward pass.\n",
    "\n",
    "        loss = criterion(out, data.y).to(device)  # Compute the loss.\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        opt.step()  # Update parameters based on gradients.\n",
    "        opt.zero_grad()  # Clear gradients.\n",
    "        pred = out.argmax(dim = 1)\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return correct / len(train_loader.dataset), total_loss / len(train_loader)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total_loss = 0.0  \n",
    "    for data, visual_feature, title, description, duration in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        \n",
    "        title = title.to(device)\n",
    "        description = description.to(device)\n",
    "        duration = duration.to(device)\n",
    "        visual_feature = visual_feature.to(device)\n",
    "#         description = bert_encoder(description).pooler_output\n",
    "#         title = bert_encoder(title).pooler_output   \n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.batch, visual_feature, title, description, duration)\n",
    "        loss = criterion(out, data.y).to(device)  # Compute the loss.\n",
    "        pred = out.argmax(dim = 1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return correct/len(loader.dataset), total_loss / len(loader) # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "\n",
    "# pbar = tqdm(range(EPOCHS), \n",
    "#             ascii = ' =')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [25:44<00:00,  3.09s/it, LR=0.0001, WD=0.001, TA=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 0.001, Train Acc : 0.8231354642313546, Valid Acc : 0.8248175182481752 , Train Loss : 0.3960486588258188, Valid Loss : 0.43129028150668514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [23:01<00:00,  2.76s/it, LR=0.0001, WD=0.0001, TA=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 0.0001, Train Acc : 0.8925418569254185, Valid Acc : 0.8564476885644768 , Train Loss : 0.2647552626225555, Valid Loss : 0.3551078817019096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:57<00:00,  2.64s/it, LR=0.0001, WD=1e-5, TA=0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 1e-05, Train Acc : 0.8946727549467276, Valid Acc : 0.8564476885644768 , Train Loss : 0.2570337274482528, Valid Loss : 0.33669909032491535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:55<00:00,  2.63s/it, LR=1e-5, WD=0.001, TA=0.856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 0.001, Train Acc : 0.8039573820395738, Valid Acc : 0.8248175182481752 , Train Loss : 0.4522040434254026, Valid Loss : 0.42033936656438387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [22:00<00:00,  2.64s/it, LR=1e-5, WD=0.0001, TA=0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 0.0001, Train Acc : 0.863013698630137, Valid Acc : 0.8369829683698297 , Train Loss : 0.332840274549225, Valid Loss : 0.39361799909518314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:56<00:00,  2.63s/it, LR=1e-5, WD=1e-5, TA=0.883,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 1e-05, Train Acc : 0.878234398782344, Valid Acc : 0.8442822384428224 , Train Loss : 0.2952495159454716, Valid Loss : 0.3815749746102553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:56<00:00,  2.63s/it, LR=1e-6, WD=0.001, TA=0.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-06, WD : 0.001, Train Acc : 0.7701674277016742, Valid Acc : 0.8029197080291971 , Train Loss : 0.5429747360713274, Valid Loss : 0.4346453868425809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [22:02<00:00,  2.65s/it, LR=1e-6, WD=0.0001, TA=0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-06, WD : 0.0001, Train Acc : 0.7729071537290715, Valid Acc : 0.8077858880778589 , Train Loss : 0.541130272219482, Valid Loss : 0.4321410655975342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [22:04<00:00,  2.65s/it, LR=1e-6, WD=1e-5, TA=0.802,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-06, WD : 1e-05, Train Acc : 0.7726027397260274, Valid Acc : 0.805352798053528 , Train Loss : 0.5407651046526085, Valid Loss : 0.43190068006515503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LR_LIST = [1e-4, 1e-5, 1e-6]\n",
    "WEIGHT_DECAY = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "grid_result = pd.DataFrame(columns=['epoch', 'learning_rate', 'weight_decay', \n",
    "                                    'train_acc', 'valid_acc', \n",
    "                                    'train_loss', 'valid_loss'],\n",
    "                           index= range(len(LR_LIST) * len(WEIGHT_DECAY)))\n",
    "\n",
    "iter_ = 0\n",
    "patience = 0\n",
    "\n",
    "BEST_VALID_ACC = 0\n",
    "BEST_LR = 0\n",
    "BEST_WD = 0\n",
    "prev_valid_loss = 1e+10\n",
    "\n",
    "for lr in LR_LIST :\n",
    "    for wd in WEIGHT_DECAY :\n",
    "        model = Model().to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loss_list = list()\n",
    "        \n",
    "        valid_loss_list = list()\n",
    "\n",
    "        min_valid_loss = 2e+10\n",
    "\n",
    "        best_epoch = 0\n",
    "        pbar = tqdm(range(EPOCHS), \n",
    "                    ascii = ' =')\n",
    "        \n",
    "        for epoch in pbar :\n",
    "        #for epoch in tqdm(range(EPOCHS)) :\n",
    "            \n",
    "            time.sleep(1)\n",
    "            train_acc, train_loss = train(optimizer)\n",
    "            valid_acc, valid_loss = test(valid_loader)\n",
    "            train_loss_list.append(train_loss)\n",
    "            valid_loss_list.append(valid_loss)\n",
    "            \n",
    "            if valid_loss < min_valid_loss :\n",
    "                best_epoch = epoch\n",
    "                torch.save(model, MODEL_PATH + \"best_model_binary_{}_{}.pt\".format(lr, wd))\n",
    "                torch.save(model.state_dict(), MODEL_PATH + 'best_model_binary_parameters_{}_{}.pt'.format(lr, wd))\n",
    "                min_valid_loss = valid_loss\n",
    "                grid_result.iloc[iter_, :] = [epoch, lr, wd, \n",
    "                                              train_acc, train_loss, \n",
    "                                              valid_acc, valid_loss]\n",
    "                best_ta = train_acc\n",
    "                best_va = valid_acc\n",
    "                best_tl = train_loss\n",
    "                best_vl = valid_loss\n",
    "                \n",
    "                \n",
    "            else :\n",
    "                patience += 1        \n",
    "            pbar.set_description(f'Epoch {epoch+1} ')\n",
    "            pbar.set_postfix({'LR' : lr,\n",
    "                              'WD' : wd,\n",
    "                              'TA' : train_acc,\n",
    "                              'TL' : train_loss, \n",
    "                              'VA' : valid_acc,\n",
    "                              'VL' : valid_loss})        \n",
    "        print(\"Epoch : {}, LR : {}, WD : {}, Train Acc : {}, Valid Acc : {} , Train Loss : {}, Valid Loss : {}\".format(epoch +1, lr, wd, \n",
    "                                                                                                                       best_ta, best_va, \n",
    "                                                                                                                       best_tl, best_vl))\n",
    "        if prev_valid_loss > best_vl :\n",
    "            BEST_VALID_ACC = best_va\n",
    "            BEST_LR = lr\n",
    "            BEST_WD = wd\n",
    "            prev_valid_loss = best_vl        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LR, BEST_WD = 0.0001, 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(loader, lr, wd):\n",
    "    y_pred = list()\n",
    "    y_test = list()\n",
    "    \n",
    "    best_model = torch.load(MODEL_PATH + \"best_model_binary_{}_{}.pt\".format(lr, wd))\n",
    "    best_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data, visual_feature, title, description, duration in loader: # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        visual_feature = visual_feature.to(device)\n",
    "        title = title.to(device)\n",
    "        description = description.to(device)\n",
    "        duration = duration.to(device)\n",
    "        \n",
    "#        description = bert_encoder(description).pooler_output\n",
    "#        title = bert_encoder(title).pooler_output   \n",
    "        \n",
    "        out = best_model(data.x, data.edge_index, data.batch, visual_feature, title, description, duration)        \n",
    "\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        \n",
    "        y_pred.extend(pred.tolist())\n",
    "        y_test.extend(data.y.tolist())\n",
    "        \n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(loader, lr, wd):\n",
    "    y_pred = list()\n",
    "    y_test = list()\n",
    "    \n",
    "    best_model = torch.load(MODEL_PATH + \"best_model_binary_{}_{}.pt\".format(lr, wd))\n",
    "    best_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for idx, (data, visual_feature, title, description, duration) in enumerate(loader): # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        visual_feature = visual_feature.to(device)\n",
    "        title = title.to(device)\n",
    "        description = description.to(device)\n",
    "        duration = duration.to(device)\n",
    "        \n",
    "#        description = bert_encoder(description).pooler_output\n",
    "#        title = bert_encoder(title).pooler_output   \n",
    "        \n",
    "        out = best_model(data.x, data.edge_index, data.batch, visual_feature, title, description, duration)        \n",
    "\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        \n",
    "        y_pred.extend(pred.tolist())\n",
    "        y_test.extend(data.y.tolist())\n",
    "        if idx == 3 :\n",
    "            return y_test[-1], y_pred[-1]\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0, 42, 44, 44, 45, 67, 74],\n",
       "        [ 0, 42, 44, 67, 74,  0,  0, 44, 45,  0,  0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('person', 'fork', 'spoon', 'bowl', 'cell phone', 'clock')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0], names[42], names[44], names[45], names[67], names[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1\n",
      "Predict Label: 1\n"
     ]
    }
   ],
   "source": [
    "#500\n",
    "target_names = ['daily', 'depression']\n",
    "\n",
    "y_test, y_pred = demo(demo_loader, BEST_LR, BEST_WD)\n",
    "print(\"True Label: {}\\nPredict Label: {}\".format(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jy_gpu_py37",
   "language": "python",
   "name": "jy_gpu_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
