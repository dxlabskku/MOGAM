{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=0gZ-l0npPIca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n",
    "# pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/anaconda3/envs/jy_gpu_py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "#import scipy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from typing import Optional, Tuple\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "# with open(\"file_info.json\", 'r') as f :\n",
    "#     file_info = json.load(f)\n",
    "    \n",
    "names= ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "        'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4125, 1888, 2237)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '/media/dxlab/storage/junyeop/'\n",
    "#len(os.listdir(PATH + \"image_dep\")), len(os.listdir(PATH + \"image_daily\"))\n",
    "\n",
    "adj_daily = os.listdir(PATH + \"adj_mat/daily/\")\n",
    "adj_depression = os.listdir(PATH + \"adj_mat/depression/\")\n",
    "adj_diagnosis = os.listdir(PATH + \"adj_mat/diagnosis/\")\n",
    "\n",
    "\n",
    "with open(PATH + 'vlog_metadata_diagnosis.pickle', 'rb') as f :\n",
    "    meta_diagnosis = pickle.load(f)\n",
    "\n",
    "with open(PATH + 'vlog_metadata_daily.pickle', 'rb') as f :\n",
    "    meta_daily = pickle.load(f)\n",
    "with open(PATH + 'vlog_metadata_daily2.pickle', 'rb') as f :\n",
    "    meta_daily2= pickle.load(f)    \n",
    "meta_daily.update(meta_daily2)\n",
    "\n",
    "with open(PATH + 'vlog_backup/vlog_dep_det/vlog_metadata_description.pickle', 'rb') as f :\n",
    "    meta_depression = pickle.load(f)\n",
    "\n",
    "\n",
    "file_info = dict()\n",
    "\n",
    "have_meta_daily = dict()\n",
    "for i in adj_daily :\n",
    "    if os.path.splitext(i)[0] in list(meta_daily.keys()) :\n",
    "        if type(meta_daily[os.path.splitext(i)[0]]) == str :\n",
    "            continue\n",
    "        have_meta_daily[os.path.splitext(i)[0]] = meta_daily[os.path.splitext(i)[0]]\n",
    "        have_meta_daily[os.path.splitext(i)[0]]['label'] = 0\n",
    "        \n",
    "# have_meta_depression = dict()\n",
    "# for i in adj_depression :\n",
    "#     if os.path.splitext(i)[0] in list(meta_depression.keys()) :\n",
    "#         if type(meta_depression[os.path.splitext(i)[0]]) == str :\n",
    "#             continue\n",
    "#         have_meta_depression[os.path.splitext(i)[0]] = meta_depression[os.path.splitext(i)[0]]\n",
    "#         have_meta_depression[os.path.splitext(i)[0]]['label'] = 1\n",
    "        \n",
    "have_meta_diagnosis = dict()\n",
    "for i in adj_diagnosis :\n",
    "    if os.path.splitext(i)[0] in list(meta_diagnosis.keys()) :\n",
    "        if type(meta_diagnosis[os.path.splitext(i)[0]]) == str :\n",
    "            continue        \n",
    "        have_meta_diagnosis[os.path.splitext(i)[0]] = meta_diagnosis[os.path.splitext(i)[0]]\n",
    "        have_meta_diagnosis[os.path.splitext(i)[0]]['label'] = 1\n",
    "        \n",
    "file_info = dict()\n",
    "file_info.update(have_meta_daily)\n",
    "#file_info.update(have_meta_depression)\n",
    "file_info.update(have_meta_diagnosis)\n",
    "        \n",
    "len(file_info), len(have_meta_daily), len(have_meta_diagnosis) #len(have_meta_depression), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "file_info = list(file_info.items())\n",
    "random.shuffle(file_info)\n",
    "file_info = dict(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_ATTR = 80\n",
    "HIDDEN_DIM = 1024\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = torch.eye(len(names), dtype = torch.float32) # node feature matrix all one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily vlog : 1888\n",
      "diagnosis vlog : 2237\n"
     ]
    }
   ],
   "source": [
    "labels = list()\n",
    "c0, c1, c2 = 0, 0, 0\n",
    "\n",
    "for x in file_info :\n",
    "    if file_info[x]['label'] == 0:\n",
    "        labels.append(0)\n",
    "        c0 += 1\n",
    "#     elif file_info[x]['label'] == 1 :\n",
    "#         labels.append(1)\n",
    "#         c1 += 1\n",
    "    else :\n",
    "        labels.append(1)\n",
    "        c2 += 1 \n",
    "labels = torch.LongTensor(labels).unsqueeze(dim = 1)\n",
    "print(\"daily vlog : {}\\ndiagnosis vlog : {}\".format(c0, c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4125it [00:03, 1065.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = list()\n",
    "\n",
    "for idx, mat_dir in tqdm(enumerate(file_info)) :\n",
    "\n",
    "    if labels[idx] == 0 :\n",
    "        with open(PATH + \"adj_mat/daily/\" + mat_dir + '.npy', 'rb') as f:\n",
    "            m = np.load(f)\n",
    "#     elif labels[idx] == 1:\n",
    "#         with open(PATH + \"adj_mat/depression/\" + mat_dir + '.npy', 'rb') as f:\n",
    "#             m = np.load(f)\n",
    "    else :\n",
    "        with open(PATH + \"adj_mat/diagnosis/\" + mat_dir + '.npy', 'rb') as f:\n",
    "            m = np.load(f)        \n",
    "    m = nx.adjacency_matrix(nx.from_numpy_array(m))\n",
    "    edge_index, edge_attr = torch_geometric.utils.from_scipy_sparse_matrix(m)\n",
    "    data = Data(edge_index=edge_index, x=feature_matrix, edge_attr = edge_attr, y = labels[idx])\n",
    "    #data = Data(edge_index=edge_index,  edge_attr = edge_attr, y = labels[idx])\n",
    "    dataset.append(data)\n",
    "    \n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
    "valid_dataset = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
    "test_dataset = dataset[int(len(dataset)*0.9):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [x.y for x in train_dataset]\n",
    "y_valid = [x.y for x in valid_dataset]\n",
    "y_test = [x.y for x in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = HIDDEN_DIM, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
    "\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
    "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \n",
    "        batch_size = value.size(0)\n",
    "\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n",
    "\n",
    "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
    "\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n",
    "\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(Model, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(NODE_ATTR, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)        \n",
    "        \n",
    "        \n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels//2)\n",
    "        self.lin2 = Linear(hidden_channels//2, hidden_channels//4)\n",
    "        \n",
    "        self.classifier = Linear(hidden_channels//4, len(labels.unique()))\n",
    "        \n",
    "        self.attn = MultiHeadAttention()\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        \n",
    "        # 3. self Attention layer\n",
    "        x = self.attn(x, x, x)[0]\n",
    "\n",
    "        # 5. Apply a final classifier\n",
    "        x = F.dropout(self.lin1(x), p=0.5, training=self.training)\n",
    "        x = F.dropout(self.lin2(x), p=0.5, training=self.training)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        \n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "\n",
    "        \n",
    "        loss = criterion(out, data.y).to(device)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        opt.step()  # Update parameters based on gradients.\n",
    "        opt.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        pred = out.argmax(dim = 1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    \n",
    "    return correct / len(train_loader.dataset), total_loss / len(train_loader)\n",
    "    \n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total_loss = 0.0  \n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y).to(device)  # Compute the loss.\n",
    "        \n",
    "        pred = out.argmax(dim = 1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return correct/len(loader.dataset), total_loss / len(loader)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Model(hidden_channels=HIDDEN_DIM).to(device)\n",
    "#model = GCN(hidden_channels=HIDDEN_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:23<00:00,  2.57s/it, LR=0.0001, WD=0.001, TA=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 0.001, Train Acc : 0.8675757575757576, Valid Acc : 0.808252427184466 , Train Loss : 0.3158948878542735, Valid Loss : 0.4119804914181049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:22<00:00,  2.56s/it, LR=0.0001, WD=0.0001, TA=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 0.0001, Train Acc : 0.8451515151515151, Valid Acc : 0.8106796116504854 , Train Loss : 0.3503010215667578, Valid Loss : 0.42245798386060274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:19<00:00,  2.56s/it, LR=0.0001, WD=1e-5, TA=0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 0.0001, WD : 1e-05, Train Acc : 0.8563636363636363, Valid Acc : 0.8203883495145631 , Train Loss : 0.33228161400900436, Valid Loss : 0.42562095935528094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:25<00:00,  2.57s/it, LR=1e-5, WD=0.001, TA=0.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 0.001, Train Acc : 0.8578787878787879, Valid Acc : 0.8009708737864077 , Train Loss : 0.3388361965234463, Valid Loss : 0.41727601564847505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:26<00:00,  2.57s/it, LR=1e-5, WD=0.0001, TA=0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 0.0001, Train Acc : 0.8593939393939394, Valid Acc : 0.808252427184466 , Train Loss : 0.3319491443152611, Valid Loss : 0.42014507146982044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:32<00:00,  2.59s/it, LR=1e-5, WD=1e-5, TA=0.908,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-05, WD : 1e-05, Train Acc : 0.863030303030303, Valid Acc : 0.8131067961165048 , Train Loss : 0.3255628916220023, Valid Loss : 0.4225354492664337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500 : 100%|=| 500/500 [21:32<00:00,  2.59s/it, LR=1e-6, WD=0.001, TA=0.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, LR : 1e-06, WD : 0.001, Train Acc : 0.8051515151515152, Valid Acc : 0.7961165048543689 , Train Loss : 0.42466455397124475, Valid Loss : 0.4662620723247528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 479 :  96%| | 478/500 [19:26<00:51,  2.35s/it, LR=1e-6, WD=0.0001, TA=0.80IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_LIST = [1e-4, 1e-5, 1e-6]\n",
    "WEIGHT_DECAY = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "grid_result = pd.DataFrame(columns=['epoch', 'learning_rate', 'weight_decay', \n",
    "                                    'train_acc', 'valid_acc', \n",
    "                                    'train_loss', 'valid_loss'],\n",
    "                           index= range(len(LR_LIST) * len(WEIGHT_DECAY)))\n",
    "\n",
    "iter_ = 0\n",
    "\n",
    "for lr in LR_LIST :\n",
    "    for wd in WEIGHT_DECAY :\n",
    "        model = Model(hidden_channels=HIDDEN_DIM).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loss_list = list()\n",
    "        \n",
    "        valid_loss_list = list()\n",
    "\n",
    "        min_valid_loss = 2e+10\n",
    "\n",
    "        best_epoch = 0\n",
    "        pbar = tqdm(range(EPOCHS), \n",
    "                    ascii = ' =')\n",
    "        \n",
    "        for epoch in pbar :\n",
    "        #for epoch in tqdm(range(EPOCHS)) :\n",
    "            \n",
    "            time.sleep(1)\n",
    "            train_acc, train_loss = train(optimizer)\n",
    "            valid_acc, valid_loss = test(valid_loader)\n",
    "            train_loss_list.append(train_loss)\n",
    "            valid_loss_list.append(valid_loss)\n",
    "            \n",
    "            if valid_loss < min_valid_loss :\n",
    "                best_epoch = epoch\n",
    "                torch.save(model, \"./checkpoint/grid_search/0_binary_depression_unimodal/GAT/best_model_binary_{}_{}.pt\".format(lr, wd))\n",
    "                torch.save(model.state_dict(), './checkpoint/grid_search/0_binary_depression_unimodal/GAT/best_model_binary_parameters_{}_{}.pt'.format(lr, wd))\n",
    "                min_valid_loss = valid_loss\n",
    "                grid_result.iloc[iter_, :] = [epoch, lr, wd, \n",
    "                                              train_acc, train_loss, \n",
    "                                              valid_acc, valid_loss]\n",
    "                best_ta = train_acc\n",
    "                best_va = valid_acc\n",
    "                best_tl = train_loss\n",
    "                best_vl = valid_loss\n",
    "                \n",
    "            else :\n",
    "                pass\n",
    "            pbar.set_description(f'Epoch {epoch+1} ')\n",
    "            pbar.set_postfix({'LR' : lr,\n",
    "                              'WD' : wd,\n",
    "                              'TA' : train_acc,\n",
    "                              'TL' : train_loss, \n",
    "                              'VA' : valid_acc,\n",
    "                              'VL' : valid_loss})        \n",
    "        print(\"Epoch : {}, LR : {}, WD : {}, Train Acc : {}, Valid Acc : {} , Train Loss : {}, Valid Loss : {}\".format(epoch +1, lr, wd, \n",
    "                                                                                                                       best_ta, best_va, \n",
    "                                                                                                                       best_tl, best_vl))\n",
    "        iter_ += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(loader, lr, wd):\n",
    "    y_pred = list()\n",
    "    y_test = list()\n",
    "    best_model = torch.load(\"./checkpoint/grid_search/0_binary_depression_unimodal/GAT/best_model_binary_{}_{}.pt\".format(lr, wd))\n",
    "    best_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        \n",
    "        data.x = data.x.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        \n",
    "        out = best_model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        \n",
    "        y_pred.extend(pred.tolist())\n",
    "        y_test.extend(data.y.tolist())\n",
    "        \n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['daily', 'depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.8040    0.8040    0.8040       199\n",
      "  depression     0.8178    0.8178    0.8178       214\n",
      "\n",
      "    accuracy                         0.8111       413\n",
      "   macro avg     0.8109    0.8109    0.8109       413\n",
      "weighted avg     0.8111    0.8111    0.8111       413\n",
      "\n",
      "0.0001 0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7860    0.8492    0.8164       199\n",
      "  depression     0.8485    0.7850    0.8155       214\n",
      "\n",
      "    accuracy                         0.8160       413\n",
      "   macro avg     0.8173    0.8171    0.8160       413\n",
      "weighted avg     0.8184    0.8160    0.8160       413\n",
      "\n",
      "0.0001 1e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7981    0.8543    0.8252       199\n",
      "  depression     0.8550    0.7991    0.8261       214\n",
      "\n",
      "    accuracy                         0.8257       413\n",
      "   macro avg     0.8266    0.8267    0.8257       413\n",
      "weighted avg     0.8276    0.8257    0.8257       413\n",
      "\n",
      "1e-05 0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7892    0.8090    0.7990       199\n",
      "  depression     0.8182    0.7991    0.8085       214\n",
      "\n",
      "    accuracy                         0.8039       413\n",
      "   macro avg     0.8037    0.8041    0.8038       413\n",
      "weighted avg     0.8042    0.8039    0.8039       413\n",
      "\n",
      "1e-05 0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7799    0.8191    0.7990       199\n",
      "  depression     0.8235    0.7850    0.8038       214\n",
      "\n",
      "    accuracy                         0.8015       413\n",
      "   macro avg     0.8017    0.8021    0.8014       413\n",
      "weighted avg     0.8025    0.8015    0.8015       413\n",
      "\n",
      "1e-05 1e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7960    0.8040    0.8000       199\n",
      "  depression     0.8160    0.8084    0.8122       214\n",
      "\n",
      "    accuracy                         0.8063       413\n",
      "   macro avg     0.8060    0.8062    0.8061       413\n",
      "weighted avg     0.8064    0.8063    0.8063       413\n",
      "\n",
      "1e-06 0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7696    0.7889    0.7792       199\n",
      "  depression     0.7990    0.7804    0.7896       214\n",
      "\n",
      "    accuracy                         0.7845       413\n",
      "   macro avg     0.7843    0.7847    0.7844       413\n",
      "weighted avg     0.7849    0.7845    0.7846       413\n",
      "\n",
      "1e-06 0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7696    0.7889    0.7792       199\n",
      "  depression     0.7990    0.7804    0.7896       214\n",
      "\n",
      "    accuracy                         0.7845       413\n",
      "   macro avg     0.7843    0.7847    0.7844       413\n",
      "weighted avg     0.7849    0.7845    0.7846       413\n",
      "\n",
      "1e-06 1e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       daily     0.7696    0.7889    0.7792       199\n",
      "  depression     0.7990    0.7804    0.7896       214\n",
      "\n",
      "    accuracy                         0.7845       413\n",
      "   macro avg     0.7843    0.7847    0.7844       413\n",
      "weighted avg     0.7849    0.7845    0.7846       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#epoch 500\n",
    "for lr in LR_LIST :\n",
    "    for wd in WEIGHT_DECAY :\n",
    "        print(lr, wd)\n",
    "        y_test, y_pred = inference(test_loader, lr, wd)\n",
    "        print(classification_report(y_test, y_pred, target_names = target_names, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jy_gpu_py37",
   "language": "python",
   "name": "jy_gpu_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
